[
  {
    "question": "A company has developed several AWS Glue extract, transform, and load (ETL) jobs to validate and transform data from Amazon S3. The ETL jobs load the data into Amazon RDS for MySQL in batches once every day. The ETL jobs use a DynamicFrame to read the S3 data.<br><br>The ETL jobs currently process all the data that is in the S3 bucket. However, the company wants the jobs to process only the daily incremental data.<br><br>Which solution will meet this requirement with the LEAST coding effort?",
    "options": [
      "Create an ETL job that reads the S3 file status and logs the status in Amazon DynamoDB.",
      "Enable job bookmarks for the ETL jobs to update the state after a run to keep track of previously processed data.",
      "Enable job metrics for the ETL jobs to help keep track of processed objects in Amazon CloudWatch.",
      "Configure the ETL jobs to delete processed objects from Amazon S3 after each run."
    ],
    "answer": "B",
    "type": "single"
  },
  {
    "question": "An online retail company has an application that runs on Amazon EC2 instances that are in a VPC. The company wants to collect flow logs for the VPC and analyze network traffic.<br><br>Which solution will meet these requirements MOST cost-effectively?",
    "options": [
      "Publish flow logs to Amazon CloudWatch Logs. Use Amazon Athena for analytics.",
      "Publish flow logs to Amazon CloudWatch Logs. Use an Amazon OpenSearch Service cluster for analytics.",
      "Publish flow logs to Amazon S3 in text format. Use Amazon Athena for analytics.",
      "Publish flow logs to Amazon S3 in Apache Parquet format. Use Amazon Athena for analytics."
    ],
    "answer": "D",
    "type": "single"
  },
  {
    "question": "A retail company stores transactions, store locations, and customer information tables in four reserved ra3.4xlarge Amazon Redshift cluster nodes. All three tables use even table distribution.<br><br>The company updates the store location table only once or twice every few years.<br><br>A data engineer notices that Redshift queues are slowing down because the whole store location table is constantly being broadcast to all four compute nodes for most queries. The data engineer wants to speed up the query performance by minimizing the broadcasting of the store location table.<br><br>Which solution will meet these requirements in the MOST cost-effective way?",
    "options": [
      "Change the distribution style of the store location table from EVEN distribution to ALL distribution.",
      "Change the distribution style of the store location table to KEY distribution based on the column that has the highest dimension.",
      "Add a join column named store_id into the sort key for all the tables.",
      "Upgrade the Redshift reserved node to a larger instance size in the same instance family."
    ],
    "answer": "A",
    "type": "single"
  },
  {
    "question": "A company has a data warehouse that contains a table that is named Sales. The company stores the table in Amazon Redshift. The table includes a column that is named city_name. The company wants to query the table to find all rows that have a city_name that starts with \"San\" or \"El\".<br><br>Which SQL query will meet this requirement?",
    "options": [
      "Select * from Sales where city_name ~ ‘$(San|El)*’;",
      "Select * from Sales where city_name ~ ‘^(San|El)*’;",
      "Select * from Sales where city_name ~’$(San&El)*’;",
      "Select * from Sales where city_name ~ ‘^(San&El)*’;"
    ],
    "answer": "B",
    "type": "single"
  },
  {
    "question": "A company needs to send customer call data from its on-premises PostgreSQL database to AWS to generate near real-time insights. The solution must capture and load updates from operational data stores that run in the PostgreSQL database. The data changes continuously.<br><br>A data engineer configures an AWS Database Migration Service (AWS DMS) ongoing replication task. The task reads changes in near real time from the PostgreSQL source database transaction logs for each table. The task then sends the data to an Amazon Redshift cluster for processing.<br><br>The data engineer discovers latency issues during the change data capture (CDC) of the task. The data engineer thinks that the PostgreSQL source database is causing the high latency.<br><br>Which solution will confirm that the PostgreSQL database is the source of the high latency?",
    "options": [
      "Use Amazon CloudWatch to monitor the DMS task. Examine the CDCIncomingChanges metric to identify delays in the CDC from the source database.",
      "Verify that logical replication of the source database is configured in the postgresql.conf configuration file.",
      "Enable Amazon CloudWatch Logs for the DMS endpoint of the source database. Check for error messages.",
      "Use Amazon CloudWatch to monitor the DMS task. Examine the CDCLatencySource metric to identify delays in the CDC from the source database."
    ],
    "answer": "D",
    "type": "single"
  },
  {
    "question": "A lab uses IoT sensors to monitor humidity, temperature, and pressure for a project. The sensors send 100 KB of data every 10 seconds. A downstream process will read the data from an Amazon S3 bucket every 30 seconds.<br><br>Which solution will deliver the data to the S3 bucket with the LEAST latency?",
    "options": [
      "Use Amazon Kinesis Data Streams and Amazon Kinesis Data Firehose to deliver the data to the S3 bucket. Use the default buffer interval for Kinesis Data Firehose.",
      "Use Amazon Kinesis Data Streams to deliver the data to the S3 bucket. Configure the stream to use 5 provisioned shards.",
      "Use Amazon Kinesis Data Streams and call the Kinesis Client Library to deliver the data to the S3 bucket. Use a 5 second buffer interval from an application.",
      "Use Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) and Amazon Kinesis Data Firehose to deliver the data to the S3 bucket. Use a 5 second buffer interval for Kinesis Data Firehose."
    ],
    "answer": "C",
    "type": "single"
  },
  {
    "question": "A company wants to use machine learning (ML) to perform analytics on data that is in an Amazon S3 data lake. The company has two data transformation requirements that will give consumers within the company the ability to create reports.<br><br>The company must perform daily transformations on 300 GB of data that is in a variety format that must arrive in Amazon S3 at a scheduled time. The company must perform one-time transformations of terabytes of archived data that is in the S3 data lake. The company uses Amazon Managed Workflows for Apache Airflow (Amazon MWAA) Directed Acyclic Graphs (DAGs) to orchestrate processing.<br><br>Which combination of tasks should the company schedule in the Amazon MWAA DAGs to meet these requirements MOST cost-effectively? (Choose two.)",
    "options": [
      "For daily incoming data, use AWS Glue crawlers to scan and identify the schema.",
      "For daily incoming data, use Amazon Athena to scan and identify the schema.",
      "For daily incoming data, use Amazon Redshift to perform transformations.",
      "For daily and archived data, use Amazon EMR to perform data transformations.",
      "For archived data, use Amazon SageMaker to perform data transformations."
    ],
    "answer": [
      "A",
      "D"
    ],
    "type": "multiple"
  },
  {
    "question": "A retail company uses AWS Glue for extract, transform, and load (ETL) operations on a dataset that contains information about customer orders. The company wants to implement specific validation rules to ensure data accuracy and consistency.<br><br>Which solution will meet these requirements?",
    "options": [
      "Use AWS Glue job bookmarks to track the data for accuracy and consistency.",
      "Create custom AWS Glue Data Quality rulesets to define specific data quality checks.",
      "Use the built-in AWS Glue Data Quality transforms for standard data quality validations.",
      "Use AWS Glue Data Catalog to maintain a centralized data schema and metadata repository."
    ],
    "answer": "B",
    "type": "single"
  }
]